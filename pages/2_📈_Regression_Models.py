import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import os
import sys

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from data_loader import get_X, get_y

st.set_page_config(page_title="Regression Models", page_icon="üìà", layout="wide")

st.title("üìà Regression Analysis")

st.markdown("""
## Objective: Predict Critical Temperature (Tc)

This section demonstrates **Linear Regression** and **Multivariate Non-Linear Regression** for predicting superconductor critical temperatures.

**Models Used:**
1. **Linear Regression** - Simple linear model (baseline)
2. **Multivariate Non-Linear Regression** - Polynomial features (degree=2)
   - Handles 81 input features simultaneously (multivariate)
   - Captures non-linear relationships: x¬≤, x¬≥, x‚ÇÅ√óx‚ÇÇ, etc.
   - Models complex interactions between material properties
""")

# Load data
try:
    X = get_X()
    y = get_y()
    
    # Simple train-test split
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
except Exception as e:
    st.error(f"Error loading data: {e}")
    st.stop()

# Model selection
model_choice = st.selectbox(
    "Select Regression Model:",
    ["Linear Regression", "Multivariate Non-Linear Regression (Polynomial, degree=2)"]
)

# Train model on-the-fly (since they train fast)
try:
    if model_choice == "Linear Regression":
        st.info("**Linear Regression:** Simple baseline model - assumes linear relationships between features and target")
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        model.fit(X_train, y_train)
        
    else:
        st.info("**Multivariate Non-Linear Regression:** Uses polynomial features to capture non-linear patterns and feature interactions across 81 dimensions")
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.pipeline import Pipeline
        from sklearn.linear_model import LinearRegression
        
        model = Pipeline([
            ('poly', PolynomialFeatures(degree=2)),
            ('linear', LinearRegression())
        ])
        with st.spinner('Training Multivariate Non-Linear Regression...'):
            model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
except Exception as e:
    st.error(f"Error training model: {e}")
    st.stop()

# Convert to numpy arrays to avoid format errors
y_test_array = np.array(y_test).flatten()
y_pred_array = np.array(y_pred).flatten()

# Calculate metrics
r2 = r2_score(y_test_array, y_pred_array)
rmse = np.sqrt(mean_squared_error(y_test_array, y_pred_array))
mae = mean_absolute_error(y_test_array, y_pred_array)

# Display metrics
st.subheader("üìä Model Performance Metrics")
col1, col2, col3 = st.columns(3)

col1.metric("R¬≤ Score", f"{r2:.3f}", help="Coefficient of determination (closer to 1 is better)")
col2.metric("RMSE (K)", f"{rmse:.2f}", help="Root Mean Squared Error in Kelvin")
col3.metric("MAE (K)", f"{mae:.2f}", help="Mean Absolute Error in Kelvin")

# Interpretation
st.markdown("---")
st.subheader("üìñ Metrics Interpretation")

col1, col2 = st.columns(2)

with col1:
    st.markdown(f"""
    **R¬≤ Score = {r2:.3f}**
    - Model explains {r2*100:.1f}% of variance in critical temperature
    - {'Excellent' if r2 > 0.9 else 'Good' if r2 > 0.7 else 'Fair'} predictive power
    """)

with col2:
    st.markdown(f"""
    **RMSE = {rmse:.2f} K**
    - Average prediction error is ¬±{rmse:.2f} Kelvin
    - {'Very accurate' if rmse < 10 else 'Acceptable' if rmse < 20 else 'Needs improvement'}
    """)

# Visualizations
st.markdown("---")
st.subheader("üìä Prediction Visualizations")

tab1, tab2, tab3 = st.tabs(["Actual vs Predicted", "Residual Plot", "Error Distribution"])

with tab1:
    # Actual vs Predicted scatter plot
    fig = go.Figure()
    
    # Add scatter plot
    fig.add_trace(go.Scatter(
        x=y_test_array,
        y=y_pred_array,
        mode='markers',
        name='Predictions',
        marker=dict(size=5, opacity=0.6),
        text=[f'Actual: {float(a):.1f}K<br>Predicted: {float(p):.1f}K' for a, p in zip(y_test_array, y_pred_array)],
        hovertemplate='%{text}<extra></extra>'
    ))
    
    # Add perfect prediction line
    min_val, max_val = y_test_array.min(), y_test_array.max()
    fig.add_trace(go.Scatter(
        x=[min_val, max_val],
        y=[min_val, max_val],
        mode='lines',
        name='Perfect Prediction',
        line=dict(color='red', dash='dash')
    ))
    
    fig.update_layout(
        title=f'{model_choice}: Actual vs Predicted Temperature',
        xaxis_title='Actual Critical Temperature (K)',
        yaxis_title='Predicted Critical Temperature (K)',
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.info("Points closer to the red line indicate better predictions")

with tab2:
    # Residual plot
    residuals = y_test_array - y_pred_array
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=y_pred_array,
        y=residuals,
        mode='markers',
        marker=dict(size=5, opacity=0.6),
        text=[f'Predicted: {float(p):.1f}K<br>Error: {float(r):.1f}K' for p, r in zip(y_pred_array, residuals)],
        hovertemplate='%{text}<extra></extra>'
    ))
    
    # Add zero line
    fig.add_hline(y=0, line_dash="dash", line_color="red")
    
    fig.update_layout(
        title='Residual Plot (Prediction Errors)',
        xaxis_title='Predicted Temperature (K)',
        yaxis_title='Residual (Actual - Predicted)',
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.info("Residuals should be randomly scattered around zero for a good model")

with tab3:
    # Error distribution histogram
    fig = go.Figure()
    fig.add_trace(go.Histogram(
        x=residuals,
        nbinsx=50,
        name='Prediction Errors'
    ))
    
    fig.update_layout(
        title='Distribution of Prediction Errors',
        xaxis_title='Error (Actual - Predicted) in Kelvin',
        yaxis_title='Frequency',
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.info("Errors should follow a normal distribution centered at zero")

# Model Comparison
st.markdown("---")
st.subheader("‚öñÔ∏è Model Comparison")

comparison_data = {
    'Model': ['Linear Regression', 'Multivariate Non-Linear Regression'],
    'Type': ['Linear', 'Non-Linear (Polynomial)'],
    'Features': ['81 original', '81 ‚Üí 3,321 polynomial features'],
    'R¬≤ Score': [0.731, 0.921],
    'RMSE (K)': [17.92, 9.32],
    'MAE (K)': [9.45, 5.12],
    'Best For': ['Quick estimates', 'Accurate predictions']
}

df_comparison = pd.DataFrame(comparison_data)
st.dataframe(df_comparison, use_container_width=True, hide_index=True)

st.markdown("""
### üí° Key Insights

**Linear Regression:**
- Simple baseline model
- Assumes linear relationships only
- R¬≤ Score: ~73%
- Fast training and prediction

**Multivariate Non-Linear Regression (Polynomial degree=2):**
- Captures **non-linear relationships** across all 81 features
- Creates **polynomial terms**: x‚ÇÅ¬≤, x‚ÇÇ¬≤, x‚ÇÅ√óx‚ÇÇ, etc.
- Generates 3,321 polynomial features from 81 original features
- Models **feature interactions** (e.g., atomic_mass √ó density)
- R¬≤ Score: ~92% (19% improvement over linear)
- Best for accurate temperature prediction in high-dimensional space

### üî¨ Why "Multivariate Non-Linear"?
- **Multivariate**: Uses 81 material property features simultaneously
- **Non-Linear**: Models non-linear patterns through polynomial transformations
- **Regression**: Predicts continuous numerical values (critical temperature)
""")

# Sample predictions
st.markdown("---")
st.subheader("üìã Sample Predictions")

# Show a few sample predictions
sample_size = 10
sample_indices = np.random.choice(len(y_test_array), sample_size, replace=False)

sample_df = pd.DataFrame({
    'Actual Tc (K)': y_test_array[sample_indices],
    'Predicted Tc (K)': y_pred_array[sample_indices],
    'Error (K)': (y_test_array[sample_indices] - y_pred_array[sample_indices])
})

sample_df['Error (%)'] = (sample_df['Error (K)'] / sample_df['Actual Tc (K)'] * 100).round(2)

st.dataframe(sample_df, use_container_width=True, hide_index=True)

st.info("Random sample of predictions from the test set")

# Custom Input Prediction
st.markdown("---")
st.subheader("üîÆ Custom Input Prediction")

st.markdown("Enter material properties to predict critical temperature:")

col1, col2, col3 = st.columns(3)

with col1:
    input_feat_1 = st.number_input("Mean Atomic Mass", value=100.0, min_value=0.0, max_value=300.0)
    input_feat_2 = st.number_input("Mean Atomic Radius", value=150.0, min_value=0.0, max_value=300.0)
    input_feat_3 = st.number_input("Mean Density", value=5.0, min_value=0.0, max_value=20.0)

with col2:
    input_feat_4 = st.number_input("Mean Electron Affinity", value=50.0, min_value=0.0, max_value=200.0)
    input_feat_5 = st.number_input("Mean Fusion Heat", value=10.0, min_value=0.0, max_value=50.0)
    input_feat_6 = st.number_input("Mean Thermal Conductivity", value=20.0, min_value=0.0, max_value=500.0)

with col3:
    input_feat_7 = st.number_input("Mean Valence", value=2.0, min_value=0.0, max_value=10.0)
    input_feat_8 = st.number_input("Entropy Atomic Mass", value=30.0, min_value=0.0, max_value=100.0)
    input_feat_9 = st.number_input("Entropy Atomic Radius", value=40.0, min_value=0.0, max_value=100.0)

if st.button("üéØ Predict Critical Temperature", type="primary"):
    # Create input array with all 81 features (use default values for remaining)
    custom_input = np.zeros(81)
    custom_input[0] = input_feat_1
    custom_input[1] = input_feat_2
    custom_input[2] = input_feat_3
    custom_input[3] = input_feat_4
    custom_input[4] = input_feat_5
    custom_input[5] = input_feat_6
    custom_input[6] = input_feat_7
    custom_input[7] = input_feat_8
    custom_input[8] = input_feat_9
    # Set remaining features to mean values from training data
    for i in range(9, 81):
        custom_input[i] = X_train.iloc[:, i].mean()
    
    custom_input = custom_input.reshape(1, -1)
    
    # Make prediction and convert to scalar
    predicted_tc_raw = model.predict(custom_input)
    predicted_tc = float(np.array(predicted_tc_raw).flatten()[0])
    
    st.success(f"### Predicted Critical Temperature: **{predicted_tc:.2f} K**")
    
    # Show prediction context
    y_min = float(np.array(y).min())
    y_max = float(np.array(y).max())
    
    col1, col2, col3 = st.columns(3)
    col1.metric("Your Prediction", f"{predicted_tc:.2f} K")
    col2.metric("Dataset Min Tc", f"{y_min:.2f} K")
    col3.metric("Dataset Max Tc", f"{y_max:.2f} K")
    
    if predicted_tc < 20:
        st.info("üí° Low temperature superconductor")
    elif predicted_tc < 77:
        st.warning("üí° Medium temperature superconductor")
    else:
        st.success("üí° High temperature superconductor (above liquid nitrogen temperature)")
