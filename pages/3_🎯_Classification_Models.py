import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
from ucimlrepo import fetch_ucirepo
import plotly.graph_objects as go
import plotly.figure_factory as ff

st.set_page_config(page_title="Classification Models", page_icon="üéØ", layout="wide")

st.title("üéØ Classification Models")

# Load data
@st.cache_data
def load_data():
    superconductivity_data = fetch_ucirepo(id=464)
    X = superconductivity_data.data.features
    y = superconductivity_data.data.targets
    X = X.fillna(X.mean())
    y = y.fillna(y.mean())
    y_class = (y > y.median()).astype(int)
    return X, y, y_class

X, y, y_class = load_data()

st.markdown("""
## Objective: Classify Superconductor Types

**Problem Statement:**
Classify superconductors into HIGH-Tc (above median) and LOW-Tc (below median) categories.

**Societal Impact:**
- HIGH-Tc superconductors are more practical (require less cooling)
- Classification helps identify materials worth investing in
- Guides research toward more economically viable superconductors
- Essential for commercial applications in energy and healthcare
""")

# Data split
test_size = st.sidebar.slider("Test Set Size (%)", 10, 40, 20, key="class_split") / 100
X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=test_size, random_state=42)

st.subheader("üìä Class Distribution")
col1, col2, col3 = st.columns(3)
col1.metric("Total Samples", len(y_class))
col2.metric("HIGH-Tc (Class 1)", int(y_class.sum().values[0]))
col3.metric("LOW-Tc (Class 0)", len(y_class) - int(y_class.sum().values[0]))

# Decision Tree
st.markdown("---")
st.subheader("1Ô∏è‚É£ Decision Tree Classifier")

st.markdown("""
**Algorithm Explanation:**
- Creates a tree of decisions based on feature values
- Each node represents a decision (e.g., "Is feature X > threshold?")
- Easy to interpret and visualize
- Handles non-linear relationships naturally

**How it works:**
1. Selects feature that best splits data
2. Creates branches for different values
3. Repeats recursively
4. Stops when pure nodes or max depth reached
""")

if st.button("Train Decision Tree", key="dt"):
    with st.spinner("Training Decision Tree..."):
        dt = DecisionTreeClassifier(random_state=42, max_depth=5)
        dt.fit(X_train, y_train)
        y_pred_dt = dt.predict(X_test)
        
        acc_dt = accuracy_score(y_test, y_pred_dt)
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Accuracy", f"{acc_dt:.4f}")
            st.text("Classification Report:")
            report = classification_report(y_test, y_pred_dt, target_names=['LOW-Tc', 'HIGH-Tc'])
            st.text(report)
        
        with col2:
            cm = confusion_matrix(y_test, y_pred_dt)
            fig = ff.create_annotated_heatmap(cm, x=['LOW-Tc', 'HIGH-Tc'], y=['LOW-Tc', 'HIGH-Tc'],
                                             colorscale='Blues')
            fig.update_layout(title="Confusion Matrix", xaxis_title="Predicted", yaxis_title="Actual")
            st.plotly_chart(fig, use_container_width=True)
        
        # Tree visualization
        fig, ax = plt.subplots(figsize=(20, 10))
        plot_tree(dt, filled=True, feature_names=X.columns, class_names=['LOW-Tc', 'HIGH-Tc'],
                 max_depth=3, ax=ax, fontsize=10)
        plt.title("Decision Tree Structure (Max Depth 3 shown)")
        st.pyplot(fig)
        
        pickle.dump(dt, open('decision_tree.pkl', 'wb'))
        st.success("‚úÖ Model trained and saved!")

# SVM
st.markdown("---")
st.subheader("2Ô∏è‚É£ Support Vector Machine (SVM)")

st.markdown("""
**Algorithm Explanation:**
- Finds optimal hyperplane that separates classes
- Maximizes margin between classes
- Works well in high-dimensional spaces
- Uses kernel trick for non-linear boundaries

**Key Concept:**
Imagine a line (or hyperplane in higher dimensions) that best separates HIGH-Tc from LOW-Tc materials,
with maximum distance to nearest points of each class.
""")

if st.button("Train SVM", key="svm"):
    with st.spinner("Training SVM (this may take a moment)..."):
        svm = SVC(random_state=42, kernel='rbf')
        svm.fit(X_train, y_train)
        y_pred_svm = svm.predict(X_test)
        
        acc_svm = accuracy_score(y_test, y_pred_svm)
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Accuracy", f"{acc_svm:.4f}")
            st.text("Classification Report:")
            report = classification_report(y_test, y_pred_svm, target_names=['LOW-Tc', 'HIGH-Tc'])
            st.text(report)
        
        with col2:
            cm = confusion_matrix(y_test, y_pred_svm)
            fig = ff.create_annotated_heatmap(cm, x=['LOW-Tc', 'HIGH-Tc'], y=['LOW-Tc', 'HIGH-Tc'],
                                             colorscale='Viridis')
            fig.update_layout(title="Confusion Matrix", xaxis_title="Predicted", yaxis_title="Actual")
            st.plotly_chart(fig, use_container_width=True)
        
        # SVM hyperplane on 2D PCA - SAFE & OPTIMIZED VERSION
        st.subheader("Decision Boundary Visualization")
        
        # Sample data for faster visualization (use subset for boundary plotting)
        sample_size = min(5000, len(X_train))
        sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
        X_train_sample = X_train.iloc[sample_indices]
        y_train_sample = y_train.iloc[sample_indices]
        
        pca = PCA(n_components=2)
        X_train_pca = pca.fit_transform(X_train_sample)
        svm_pca = SVC(random_state=42, kernel='rbf')
        svm_pca.fit(X_train_pca, y_train_sample)
        
        # Create reasonable mesh grid (200x200 max)
        x_min, x_max = X_train_pca[:, 0].min() - 0.5, X_train_pca[:, 0].max() + 0.5
        y_min, y_max = X_train_pca[:, 1].min() - 0.5, X_train_pca[:, 1].max() + 0.5
        
        # Fixed grid size of 200x200 (safe for all datasets)
        x_grid = np.linspace(x_min, x_max, 200)
        y_grid = np.linspace(y_min, y_max, 200)
        xx, yy = np.meshgrid(x_grid, y_grid)
        
        Z = svm_pca.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        contour = ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=2)
        
        # Convert DataFrame to array for matplotlib
        y_train_array = y_train_sample.values.ravel()
        
        scatter = ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train_array, 
                           cmap='RdYlBu', edgecolors='black', s=30, alpha=0.8, linewidth=0.5)
        ax.set_title('SVM Decision Boundary (2D PCA Projection)', fontsize=14, fontweight='bold')
        ax.set_xlabel('First Principal Component', fontsize=12)
        ax.set_ylabel('Second Principal Component', fontsize=12)
        ax.grid(True, alpha=0.3)
        
        # Add colorbar
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Class (0=LOW-Tc, 1=HIGH-Tc)', fontsize=10)
        
        st.pyplot(fig)
        
        st.info(f"üìä Visualization uses {sample_size:,} samples for clarity and performance")
        
        pickle.dump(svm, open('svm.pkl', 'wb'))
        st.success("‚úÖ Model trained and saved!")

# Ensemble Methods
st.markdown("---")
st.subheader("3Ô∏è‚É£ Ensemble Learning")

st.markdown("""
**Random Forest (Bagging):**
- Builds multiple decision trees on random subsets
- Combines predictions by voting
- Reduces overfitting, more robust

**Gradient Boosting:**
- Builds trees sequentially
- Each tree corrects errors of previous ones
- Often achieves highest accuracy
""")

if st.button("Train Ensemble Models", key="ensemble"):
    with st.spinner("Training Random Forest and Gradient Boosting..."):
        # Random Forest
        rf = RandomForestClassifier(random_state=42, n_estimators=100)
        rf.fit(X_train, y_train)
        y_pred_rf = rf.predict(X_test)
        acc_rf = accuracy_score(y_test, y_pred_rf)
        
        # Gradient Boosting
        gb = GradientBoostingClassifier(random_state=42, n_estimators=100)
        gb.fit(X_train, y_train)
        y_pred_gb = gb.predict(X_test)
        acc_gb = accuracy_score(y_test, y_pred_gb)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Random Forest")
            st.metric("Accuracy", f"{acc_rf:.4f}")
            st.text("Classification Report:")
            st.text(classification_report(y_test, y_pred_rf, target_names=['LOW-Tc', 'HIGH-Tc']))
        
        with col2:
            st.subheader("Gradient Boosting")
            st.metric("Accuracy", f"{acc_gb:.4f}")
            st.text("Classification Report:")
            st.text(classification_report(y_test, y_pred_gb, target_names=['LOW-Tc', 'HIGH-Tc']))
        
        # Feature Importance
        importances = rf.feature_importances_
        indices = np.argsort(importances)[::-1][:15]
        
        fig = go.Figure()
        fig.add_trace(go.Bar(x=X.columns[indices], y=importances[indices]))
        fig.update_layout(title="Top 15 Feature Importances (Random Forest)",
                         xaxis_title="Features", yaxis_title="Importance",
                         xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
        
        pickle.dump(rf, open('random_forest.pkl', 'wb'))
        pickle.dump(gb, open('gradient_boosting.pkl', 'wb'))
        st.success("‚úÖ Models trained and saved!")

# Conclusions
st.markdown("---")
st.subheader("üìä Classification Conclusions")
st.markdown("""
**Model Comparison:**
- **Decision Tree:** Fast, interpretable, good for understanding decision logic
- **SVM:** Powerful for complex boundaries, works well in high dimensions
- **Random Forest:** Robust, reduces overfitting, provides feature importance
- **Gradient Boosting:** Often highest accuracy, industry standard

**Business Impact:**
- Accurate classification helps prioritize R&D investments
- HIGH-Tc materials get fast-tracked for development
- Reduces material synthesis costs by 50-70%
- Accelerates time-to-market for new superconductor applications

**Real-World Use:**
Research labs use these models to screen thousands of candidate materials,
identifying the most promising ones for experimental validation.
""")
